# Meta-LLAMA-Financial-Advisor
Building an agent as a financial advisor that can analyze annual reports using Meta LLAMA2 model. 

Llama-2 released by Meta was released on 18th of July 2023 and the model is commercially available for research for the communities in different fields. In order to HuggingFace's LLAMA2 to build an application to query documents and using cost effective GPU via run pod is a task that requires several steps. In order to utilize the LLAMA2 model, and create an application, our local machines do not have the computation power to test and try the model. Therefore, extensive research was done to find a quantized version of the model on hugging face for the model. As a result of this the quantized model ‘TheBloke/Llama-2-7B-Chat-GGUF’ was used to test the model on Google Collab prior to utilizing the base model on run pod. Google Collab In order to utilize the LLM there are some libraries that is required, firstly the documents that will be uploaded will be pdf and ‘PyPDF2’ is installed to read pdf files. Then, ‘pdf2image’ is used to convert pdfs to images as well. ‘poppler-utils’ is used to assist in the processing of pdf files. ‘accelerate’ is a hugging face library that helps speed up training process of LLMs. ‘langchain’ helps with the framework for large language models for pipeline, and generation as well. ‘ctransformers’ helps when working with transformer models and assist in deploying these models. ‘torch’ is also used and ‘transformers’ from hugging face. ‘sentence transformers’ is used for embedding sentences. Chroma DP is used to query data in its raw form first converts the data into embeddings. After installing and importing the required libraries, the main objective is to process the PDF documents, generating embeddings, and setting up responses through the LLM. As the first file is uploaded, to the google Collab environment, the files are read and stored into a dictionary. There after for computing process, CPU or GPU is used. Then the embedding and text splitting is carried out, using ‘HuggingFaceInstructEmbeddings’ to create embeddings for the text. Thereafter, ‘RecursiveCharacterTextSplitter’ is used to split the text into chunks. Then, creating a chroma database, using a vector database, for storing the embeddings. It uses the text chunks and the embeddings to populate the database. Then the large language model is setup and in order to initiate the model, requires to login to Hugging Face hub and setting up a ‘CTransformers’ model with the given model ID. Thereafter, an interactive chat interface is created, using ‘ConversationalRetrievalChain’ which combines the language model and the chroma database server. Where inputs can be given and the system can respond based on the large language model from the chroma database. After testing there were several outputs for the pdf. However, a small one-page pdf document was uploaded for testing purposes. Streamlit Thereafter, using streamlit an app was developed as a chatbot. Where a custom streamlit configuration was set up and a document class is created to handle the text and meta data of each document. The PDFs are read and process the documents and incorporating hugging face token authentication and upload and process PDFs. Then setting up the chat bot along with an interactive chat interface. ‘Ngrok’ tunnel setup was used for streamlit app testing. Run Pod Run Pod provides a cloud computing options for serverless GPU computing. In order to use a pod, USD 20 was paid to the platform and based on the usage value for a pod and the hourly rate deducts from the toped-up amount (USD). Under pods, secure clod is selected and 1x RTX A400 with a 16GB VRAM and a 23 GB RAM is deployed. Since the app will be tested on streamlit the port is changed to 888,8501 for standard streamlit use. Then for the templates, Pytorch 2.1 is used.

In order to use the app in Github a repository was created with the streamlit app code as ‘.py’ file. Along with the requirements file which includes all the libraries that are required. After connecting on Run Pod, a web terminal is opened to commence the web terminal and prior to installing any libraries need to switch to the current user using ‘su –‘. Thereafter, installing sudo for and updating it. Then install nodejs and check for update in package repository. Thereafter, node package manager is installed. Then the GitHub repository is cloned in the pod. Then install all the dependencies. Then the streamlit app is launched through local tunnel port 8501. So, the app can be accessed remotely. Then after accessing the URL, under the endpoint IP the Run Pods, public IP is provided.

After launching the streamlit app, the file is uploaded and is processed. In the streamlit app.py file the setup and configuration is done and capable of document handling. Then incorporating PDF processing, the texts are extracted and combined. ‘HuggingFaceInstructEmbeddings’ creates text embeddings, ‘RecursiveCharacterTextSplitter’ splits the text into manageable chunks. ‘Chroma’ creates a vector database from the split text for efficient retrieval. Then the chat bot set up is taken place. Then the chat interface allows a user to ask relevant questions from the document.

Limitations Since a quantize version is used for this successful implementation of an application that is capable of querying documents. The base model was also tested in the same manner in order to implement the same features. However, a significant issue noted was related to memory, particularly when transitioning from LangChain's CTransformers to Hugging Face Transformers. Continued to utilize Hugging Face's Instruct embeddings. However, during the model's processing, we encountered an issue where the GPU memory was fully utilized, reaching 100%, leading to a 'Cuda has run out of memory' error. Changed the data types to FP16, which refers to the floating-point precision of tensors. This approach uses half the number of tensor units compared to the default FP32. Therefore, while the model originally operates with 32-bit floating-point precision, we are attempting to switch it to 16-bit to optimize memory usage. Furthermore, the conversational retrieval did not work as compatibility issues with the base meta llama 2 model.
